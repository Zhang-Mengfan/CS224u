{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNClassifierModel\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "import nli\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = \"/Users/bszalapski/Documents/StanfordCourses/CS224U/cs224u/data/glove.6B\"\n",
    "\n",
    "DATA_HOME = os.path.join(\"/Users/bszalapski/Documents/StanfordCourses/CS224U/cs224u_project/cs224uSNLI/data\", \"nlidata\")\n",
    "\n",
    "SNLI_HOME = os.path.join(DATA_HOME, \"snli_1.0\")\n",
    "\n",
    "MULTINLI_HOME = os.path.join(DATA_HOME, \"multinli_1.0\")\n",
    "\n",
    "ANNOTATIONS_HOME = os.path.join(DATA_HOME, \"multinli_1.0_annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = nli.SNLITrainReader(SNLI_HOME, samp_percentage=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier_with_preselected_params(X, y):       \n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True, \n",
    "        penalty='l1', \n",
    "        solver='saga',  ## Required for penalty='ll'.\n",
    "        multi_class='ovr',\n",
    "        C=0.4)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_leaves_phi(t1, t2, np_func=np.sum):\n",
    "    \"\"\"Represent `tree` as a combination of the vector of its words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1 : nltk.Tree   \n",
    "    t2 : nltk.Tree   \n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. The requirement is that \n",
    "        the function take `axis=0` as one of its arguments (to ensure\n",
    "        columnwise combination) and that it return a vector of a \n",
    "        fixed length, no matter what the size of the tree is.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "            \n",
    "    \"\"\"    \n",
    "    prem_vecs = _get_tree_vecs(t1, glove_lookup, np_func)  \n",
    "    hyp_vecs = _get_tree_vecs(t2, glove_lookup, np_func)\n",
    "    return (prem_vecs, hyp_vecs)\n",
    "    \n",
    "    \n",
    "def _get_tree_vecs(tree, lookup, np_func):\n",
    "    if hasattr(tree, 'leaves'):\n",
    "        allvecs = np.array([lookup[w] for w in tree.leaves() if w in lookup])\n",
    "    else:\n",
    "        allvecs = np.array([lookup[w] for w in tree if w in lookup])\n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)    \n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, seq_lengths, y):\n",
    "        self.prem_seqs, self.hyp_seqs = sequences\n",
    "        self.prem_lengths, self.hyp_lengths = seq_lengths\n",
    "        self.y = y\n",
    "        assert len(self.prem_seqs) == len(self.y)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        X_prem, X_hyp, prem_lengths, hyp_lengths, y = zip(*batch)\n",
    "        prem_lengths = torch.LongTensor(prem_lengths)\n",
    "        hyp_lengths = torch.LongTensor(hyp_lengths)\n",
    "        y = torch.LongTensor(y)\n",
    "        return (X_prem, X_hyp), (prem_lengths, hyp_lengths), y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prem_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.prem_seqs[idx], self.hyp_seqs[idx],\n",
    "                self.prem_lengths[idx], self.hyp_lengths[idx],\n",
    "                self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderClassifierModel(TorchRNNClassifierModel):\n",
    "    def __init__(self, vocab_size, embed_dim, embedding, use_embedding,\n",
    "                 hidden_dim, output_dim, bidirectional, device, srl_vocab_size):\n",
    "        \n",
    "        super(TorchRNNSentenceEncoderClassifierModel, self).__init__(vocab_size, embed_dim,\n",
    "                                                                     embedding, use_embedding,\n",
    "                                                                     hidden_dim, output_dim, bidirectional,\n",
    "                                                                     device)\n",
    "        self.hypothesis_rnn = nn.LSTM(\n",
    "            input_size=2 * self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        if bidirectional:\n",
    "            classifier_dim = hidden_dim * 2 * 2\n",
    "        else:\n",
    "            classifier_dim = hidden_dim * 2\n",
    "        self.classifier_layer = nn.Linear(classifier_dim, output_dim)\n",
    "        self.srl_embed = nn.Embedding(srl_vocab_size + 1, self.embed_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, X, tags, seq_lengths):\n",
    "        X_prem, X_hyp = X\n",
    "        prem_lengths, hyp_lengths = seq_lengths\n",
    "        prem_state = self.rnn_forward(X_prem, prem_lengths, self.rnn)\n",
    "        hyp_state = self.rnn_forward(X_hyp, hyp_lengths, self.hypothesis_rnn)\n",
    "        state = torch.cat((prem_state, hyp_state), dim=1)\n",
    "        logits = self.classifier_layer(state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderClassifier(TorchRNNClassifier):\n",
    "\n",
    "    def build_dataset(self, X, y):\n",
    "        X_prem, X_hyp = zip(*X)\n",
    "        X_prem, prem_lengths = self._prepare_dataset(X_prem)\n",
    "        X_hyp, hyp_lengths = self._prepare_dataset(X_hyp)\n",
    "        return TorchRNNSentenceEncoderDataset(\n",
    "            (X_prem, X_hyp), (prem_lengths, hyp_lengths), y)\n",
    "\n",
    "    def build_graph(self):\n",
    "        return TorchRNNSentenceEncoderClassifierModel(\n",
    "            len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            use_embedding=self.use_embedding,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.n_classes_,\n",
    "            bidirectional=self.bidirectional,\n",
    "            device=self.device)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        with torch.no_grad():\n",
    "            X_prem, X_hyp = zip(*X)\n",
    "            X_prem, prem_lengths = self._prepare_dataset(X_prem)\n",
    "            X_hyp, hyp_lengths = self._prepare_dataset(X_hyp)\n",
    "            preds = self.model((X_prem, X_hyp), (prem_lengths, hyp_lengths))\n",
    "            preds = torch.softmax(preds, dim=1).cpu().numpy()\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This still has bugs\n",
    "# # Adapted from: https://github.com/PrashantRanjan09/Structured-Self-Attentive-Sentence-Embedding\n",
    "# class SelfAttentiveModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, vocab_size=10000, embed_dim=300, hidden_dim=64, max_seq_len=200, da=32, r=16, batch_size=500,\n",
    "#                 embedding=None, use_embedding=True, bidirectional=True, device='cpu', output_dim=3):\n",
    "#         super(SelfAttentiveModel, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.da = da\n",
    "#         self.r = r\n",
    "#         self.device = device\n",
    "#         self.use_embedding = use_embedding\n",
    "#         self.bidirectional = bidirectional\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.bilstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
    "#         self.lin1 = nn.Linear(2 * hidden_dim, da)\n",
    "#         self.lin2 = nn.Linear(da, r)\n",
    "#         self.lin3 = nn.Linear(100, output_dim)\n",
    "\n",
    "#     def forward(self, x, seq_lengths):\n",
    "#         state = self.rnn_forward(x, seq_lengths, self.bilstm)\n",
    "#         out = self.lin1(state)\n",
    "#         out = torch.tanh(out)\n",
    "#         out = self.lin2(out)\n",
    "#         out_a = F.softmax(out, dim=0)\n",
    "#         temp1 = out_a.permute(0, 2, 1)\n",
    "#         temp2 = state\n",
    "#         out = torch.bmm(temp1, temp2).squeeze(0)  # AH\n",
    "#         out = out.view(len(x), -1)\n",
    "#         out = self.lin3(out)\n",
    "#         return out\n",
    "    \n",
    "#     def rnn_forward(self, x, seq_lengths, rnn):\n",
    "#         x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "#         x = x.to(self.device, non_blocking=True)\n",
    "#         seq_lengths = seq_lengths.to(self.device)\n",
    "#         seq_lengths, sort_idx = seq_lengths.sort(0, descending=True)\n",
    "#         x = x[sort_idx]\n",
    "#         if self.use_embedding:\n",
    "#             embs = self.embedding(x)\n",
    "#         else:\n",
    "#             embs = x\n",
    "#         embs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "#             embs, batch_first=True, lengths=seq_lengths)\n",
    "#         outputs, state = rnn(embs)\n",
    "#         state = self.get_batch_final_states(state)\n",
    "#         if self.bidirectional:\n",
    "#             state = torch.cat((state[0], state[1]), dim=1)\n",
    "#         _, unsort_idx = sort_idx.sort(0)\n",
    "#         state = state[unsort_idx]\n",
    "#         state = state.unsqueeze(0)\n",
    "#         return state\n",
    "\n",
    "#     def get_batch_final_states(self, state):\n",
    "#         if self.bilstm.__class__.__name__ == 'LSTM':\n",
    "#             return state[0].squeeze(0)\n",
    "#         else:\n",
    "#             return state.squeeze(0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _define_embedding(embedding, vocab_size, embed_dim):\n",
    "#         if embedding is None:\n",
    "#             return nn.Embedding(vocab_size, embed_dim)\n",
    "#         else:\n",
    "#             embedding = torch.tensor(embedding, dtype=torch.float)\n",
    "#             return nn.Embedding.from_pretrained(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This still has bugs\n",
    "# class SNLIBiLSTMAttentiveClassifierModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, embedding, use_embedding, hidden_dim,\n",
    "#                  output_dim, bidirectional, device, max_seq_len=200,\n",
    "#                  da=32, r=16, batch_size=500):\n",
    "#         super(SNLIBiLSTMAttentiveClassifierModel, self).__init__()\n",
    "\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.da = da\n",
    "#         self.r = r\n",
    "#         self.device = device\n",
    "#         self.use_embedding = use_embedding\n",
    "#         self.embedding = embedding\n",
    "\n",
    "#         self.prem = SelfAttentiveModel(vocab_size=vocab_size,\n",
    "#                                        embed_dim=embed_dim,\n",
    "#                                        hidden_dim=hidden_dim,\n",
    "#                                        max_seq_len=max_seq_len,\n",
    "#                                        da=da,\n",
    "#                                        r=r,\n",
    "#                                        batch_size=batch_size,\n",
    "#                                        embedding=embedding,\n",
    "#                                        use_embedding=use_embedding)\n",
    "\n",
    "#         self.hyp = SelfAttentiveModel(vocab_size=vocab_size,\n",
    "#                                       embed_dim=embed_dim,\n",
    "#                                       hidden_dim=hidden_dim,\n",
    "#                                       max_seq_len=max_seq_len,\n",
    "#                                       da=da,\n",
    "#                                       r=r,\n",
    "#                                       batch_size=batch_size,\n",
    "#                                       embedding=embedding,\n",
    "#                                       use_embedding=use_embedding)\n",
    "\n",
    "#         self.output_layer = nn.Linear(12, output_dim)\n",
    "\n",
    "#     def forward(self, x, seq_lengths):\n",
    "#         x_prem, x_hyp = x\n",
    "#         seq_length_prem, seq_length_hyp = seq_lengths\n",
    "#         encoded_prem = self.prem(x_prem, seq_length_prem)\n",
    "#         encoded_hyp = self.hyp(x_hyp, seq_length_hyp)\n",
    "\n",
    "#         mult = encoded_prem * encoded_hyp\n",
    "#         diff = encoded_prem - encoded_hyp\n",
    "#         print(encoded_prem.size(), encoded_hyp.size(), mult.size(), diff.size())\n",
    "\n",
    "#         soft_in = torch.cat((encoded_prem, mult, diff, encoded_hyp), dim=1)\n",
    "#         print(soft_in.size())\n",
    "#         print(8 * self.hidden_dim, self.output_dim)\n",
    "#         logits = self.output_layer(soft_in)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This still has bugs\n",
    "# class SNLIBiLSTMAttentionClassifier(TorchRNNClassifier):\n",
    "\n",
    "#     def build_dataset(self, X, y):\n",
    "#         X_prem, X_hyp = zip(*X)\n",
    "#         X_prem, prem_lengths = self._prepare_dataset(X_prem)\n",
    "#         X_hyp, hyp_lengths = self._prepare_dataset(X_hyp)\n",
    "#         return TorchRNNSentenceEncoderDataset(\n",
    "#             (X_prem, X_hyp), (prem_lengths, hyp_lengths), y)\n",
    "\n",
    "#     def build_graph(self):\n",
    "#         return SNLIBiLSTMAttentiveClassifierModel(\n",
    "#             len(self.vocab),\n",
    "#             embedding=self.embedding,\n",
    "#             embed_dim=self.embed_dim,\n",
    "#             use_embedding=self.use_embedding,\n",
    "#             hidden_dim=self.hidden_dim,\n",
    "#             output_dim=self.n_classes_,\n",
    "#             bidirectional=self.bidirectional,\n",
    "#             device=self.device)\n",
    "\n",
    "#     def predict_proba(self, X):\n",
    "#         with torch.no_grad():\n",
    "#             X_prem, X_hyp = zip(*X)\n",
    "#             X_prem, prem_lengths = self._prepare_dataset(X_prem)\n",
    "#             X_hyp, hyp_lengths = self._prepare_dataset(X_hyp)\n",
    "#             preds = self.model((X_prem, X_hyp), (prem_lengths, hyp_lengths))\n",
    "#             preds = torch.softmax(preds, dim=1).cpu().numpy()\n",
    "#             return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveSRLModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            vocab,\n",
    "            embed_dim,\n",
    "            embedding,\n",
    "            use_embedding,\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            bidirectional,\n",
    "            device,\n",
    "            srl_vocab,\n",
    "            lookup):\n",
    "        super(TorchRNNClassifierModel, self).__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        self.device = device\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.vocab = vocab\n",
    "        self.srl_vocab = srl_vocab\n",
    "        self.lookup = lookup\n",
    "        self.lookup_embedding = None\n",
    "        words = []\n",
    "        vecs = []\n",
    "        idx = 0\n",
    "        self.word2idx = {}\n",
    "        for w in lookup.keys():\n",
    "            words.append(w)\n",
    "            self.word2idx[w] = idx\n",
    "            idx += 1\n",
    "            vecs.append(lookup[w])\n",
    "        glove_weights = np.array(vecs)\n",
    "\n",
    "        # Graph\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(glove_weights, freeze=True)\n",
    "        self.embed_dim = self.word_embedding.embedding_dim\n",
    "        self.srl_embedding = nn.Embedding(len(srl_vocab) + 1, self.embed_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=2 * self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional)\n",
    "        \n",
    "        self.hypothesis_rnn = nn.LSTM(\n",
    "            input_size=2 * self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        if bidirectional:\n",
    "            classifier_dim = hidden_dim * 2 * 2\n",
    "        else:\n",
    "            classifier_dim = hidden_dim * 2\n",
    "        \n",
    "        self.classifier_layer = nn.Linear(classifier_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, seq_lengths):\n",
    "        prem, hyp = X\n",
    "        X_prem, tags_prem = prem\n",
    "        X_hyp, tags_hyp = hyp\n",
    "        prem_lengths, hyp_lengths = seq_lengths\n",
    "        \n",
    "        embeds_prem = torch.cat(self.word_embedding(X_prem), self.srl_embedding(tags_prem))\n",
    "        embeds_hyp = torch.cat(self.word_embedding(X_hyp), self.srl_embedding(tags_hyp))\n",
    "        \n",
    "        prem_state = self.rnn_forward(X_prem, prem_lengths, self.rnn)\n",
    "        hyp_state = self.rnn_forward(X_hyp, hyp_lengths, self.hypothesis_rnn)\n",
    "        state = torch.cat((prem_state, hyp_state), dim=1)\n",
    "        logits = self.classifier_layer(state)\n",
    "        return logits\n",
    "\n",
    "    def rnn_forward(self, X, seq_lengths, rnn):\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True)\n",
    "        X = X.to(self.device, non_blocking=True)\n",
    "        seq_lengths = seq_lengths.to(self.device)\n",
    "        seq_lengths, sort_idx = seq_lengths.sort(0, descending=True)\n",
    "        X = X[sort_idx]\n",
    "        if self.use_embedding:\n",
    "            embs = self.embedding(X)\n",
    "        else:\n",
    "            embs = X\n",
    "        embs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embs, batch_first=True, lengths=seq_lengths)\n",
    "        outputs, state = rnn(embs)\n",
    "        state = self.get_batch_final_states(state)\n",
    "        if self.bidirectional:\n",
    "            state = torch.cat((state[0], state[1]), dim=1)\n",
    "        _, unsort_idx = sort_idx.sort(0)\n",
    "        state = state[unsort_idx]\n",
    "        return state\n",
    "\n",
    "    def get_batch_final_states(self, state):\n",
    "        if self.rnn.__class__.__name__ == 'LSTM':\n",
    "            return state[0].squeeze(0)\n",
    "        else:\n",
    "            return state.squeeze(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _define_embedding(embedding, vocab_size, embed_dim):\n",
    "        if embedding is None:\n",
    "            return nn.Embedding(vocab_size, embed_dim)\n",
    "        else:\n",
    "            embedding = torch.tensor(embedding, dtype=torch.float)\n",
    "            return nn.Embedding.from_pretrained(embedding)\n",
    "        \n",
    "        def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Standard `fit` method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "        y : array-like\n",
    "        kwargs : dict\n",
    "            For passing other parameters. If 'X_dev' is included,\n",
    "            then performance is monitored every 10 epochs; use\n",
    "            `dev_iter` to control this number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        # Incremental performance:\n",
    "        X_dev = kwargs.get('X_dev')\n",
    "        if X_dev is not None:\n",
    "            dev_iter = kwargs.get('dev_iter', 10)\n",
    "        # Data prep:\n",
    "        self.classes_ = sorted(set(y))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "        y = [class2index[label] for label in y]\n",
    "        dataset = self.build_dataset(X, y)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=dataset.collate_fn)\n",
    "        if not self.use_embedding:\n",
    "            # Infer `embed_dim` from `X` in this case:\n",
    "            self.embed_dim = X[0][0].shape[0]\n",
    "        # Graph:\n",
    "        self.model = self.build_graph()\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        # Make sure this value is up-to-date; self.`model` might change\n",
    "        # it if it creates an embedding:\n",
    "        self.embed_dim = self.model.embed_dim\n",
    "        # Optimization:\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optimizer = self.optimizer(\n",
    "            self.model.parameters(),\n",
    "            lr=self.eta,\n",
    "            weight_decay=self.l2_strength)\n",
    "        # Train:\n",
    "        for iteration in range(1, self.max_iter+1):\n",
    "            epoch_error = 0.0\n",
    "            for X_batch, batch_seq_lengths, y_batch in dataloader:\n",
    "                y_batch = y_batch.to(self.device, non_blocking=True)\n",
    "                # print(self.model.__class__.__name__)\n",
    "                # print(len(X_batch), len(batch_seq_lengths))\n",
    "                batch_preds = self.model(X_batch, batch_seq_lengths)\n",
    "                err = loss(batch_preds, y_batch)\n",
    "                epoch_error += err.item()\n",
    "                # Backprop:\n",
    "                optimizer.zero_grad()\n",
    "                err.backward()\n",
    "                optimizer.step()\n",
    "            # Incremental predictions where possible:\n",
    "            if X_dev is not None and iteration > 0 and iteration % dev_iter == 0:\n",
    "                self.dev_predictions[iteration] = self.predict(X_dev)\n",
    "            self.errors.append(epoch_error)\n",
    "            progress_bar(\"Finished epoch {} of {}; error is {}\".format(\n",
    "                iteration, self.max_iter, epoch_error))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_example():\n",
    "    vocab = ['a', 'b', '$UNK']\n",
    "\n",
    "    # Reversals are good, and other pairs are bad:\n",
    "    train = [\n",
    "        [(list('ab'), list('ba')), 'good'],\n",
    "        [(list('aab'), list('baa')), 'good'],\n",
    "        [(list('abb'), list('bba')), 'good'],\n",
    "        [(list('aabb'), list('bbaa')), 'good'],\n",
    "        [(list('ba'), list('ba')), 'bad'],\n",
    "        [(list('baa'), list('baa')), 'bad'],\n",
    "        [(list('bba'), list('bab')), 'bad'],\n",
    "        [(list('bbaa'), list('bbab')), 'bad'],\n",
    "        [(list('aba'), list('bab')), 'bad']]\n",
    "\n",
    "    test = [\n",
    "        [(list('baaa'), list('aabb')), 'bad'],\n",
    "        [(list('abaa'), list('baaa')), 'bad'],\n",
    "        [(list('bbaa'), list('bbaa')), 'bad'],\n",
    "        [(list('aaab'), list('baaa')), 'good'],\n",
    "        [(list('aaabb'), list('bbaaa')), 'good']]\n",
    "\n",
    "    mod = TorchRNNSentenceEncoderClassifier(\n",
    "        vocab,\n",
    "        max_iter=100,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50)\n",
    "\n",
    "    X, y = zip(*train)\n",
    "    mod.fit(X, y)\n",
    "\n",
    "    X_test, y_test = zip(*test)\n",
    "    preds = mod.predict(X_test)\n",
    "\n",
    "    print(\"\\nPredictions:\")\n",
    "    for ex, pred, gold in zip(X_test, preds, y_test):\n",
    "        score = \"correct\" if pred == gold else \"incorrect\"\n",
    "        print(\"{0:>6} {1:>6} - predicted: {2:>4}; actual: {3:>4} - {4}\".format(\n",
    "            \"\".join(ex[0]), \"\".join(ex[1]), pred, gold, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_encoding_rnn_phi(t1, t2):\n",
    "    \"\"\"Map `t1` and `t2` to a pair of lits of leaf nodes.\"\"\"\n",
    "    if hasattr(t1, 'leaves'):\n",
    "        return (t1.leaves(), t2.leaves())\n",
    "    else:\n",
    "        return (t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "PREDICTOR = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dim = 50\n",
    "glove_src = os.path.join(GLOVE_HOME, 'glove.6B.{}d.txt'.format(glove_dim))\n",
    "# Creates a dict mapping strings (words) to GloVe vectors:\n",
    "GLOVE = utils.glove2dict(glove_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randvec(w, n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return utils.randvec(n=n, lower=lower, upper=upper)\n",
    "\n",
    "\n",
    "def glove_vec(w, lookup, is_srl=False):    \n",
    "    \"\"\"Return `w`'s GloVe representation if available, else return \n",
    "    a random vector.\"\"\"\n",
    "    vec = lookup.get(w, randvec(w, n=glove_dim))\n",
    "    if w not in lookup and is_srl:\n",
    "        print(f\"Adding {w} to lookup\")\n",
    "        lookup[w] = vec\n",
    "    return vec\n",
    "\n",
    "\n",
    "def _get_glove_vecs(s, lookup, np_func=np.concatenate, is_srl=False):\n",
    "    if hasattr(s, 'leaves'):\n",
    "        allvecs = np.array([glove_vec(w.lower(), lookup, is_srl) for w in s.leaves()])\n",
    "    else:\n",
    "        allvecs = np.array([glove_vec(w.lower(), lookup, is_srl) for w in s])\n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)    \n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats\n",
    "        \n",
    "\n",
    "def srl_phi(t1, t2):\n",
    "    \"\"\"\n",
    "    Uses the Allen NLP Semantic Role Labeler and returns the words in a list, concatenated with their SRL tag\n",
    "    e.g. t1 = [\"The\", \"cat\", \"ran\", \"fast\", \".\"]\n",
    "    then return = [\"The_O\", \"cat_B-ARG0\", \"ran_B-V\" \"fast_B-ARGM-MNR\", \"._O\"]\n",
    "    \"\"\"\n",
    "    if hasattr(t1, 'leaves'):\n",
    "        s1 = t1.leaves()\n",
    "        s2 = t2.leaves()\n",
    "    else:\n",
    "        s1 = t1\n",
    "        s2 = t2\n",
    "    srl1 = PREDICTOR.predict(sentence=\" \".join(s1))\n",
    "    srl2 = PREDICTOR.predict(sentence=\" \".join(s2))\n",
    "    tags_1 = _get_best_tags(srl1)\n",
    "    tags_2 = _get_best_tags(srl2)\n",
    "    glove_1 = _get_glove_vecs(s1, GLOVE)\n",
    "    glove_2 = _get_glove_vecs(s2, GLOVE)\n",
    "    glove_tags_1 = _get_glove_vecs(tags_1, GLOVE, is_srl=True)\n",
    "    glove_tags_2 = _get_glove_vecs(tags_2, GLOVE, is_srl=True)\n",
    "\n",
    "#     ret_1 = [np.concatenate((a_, b_), axis=0) for a_, b_ in zip(glove_1, glove_tags_1)]\n",
    "#     ret_2 = [np.concatenate((a_, b_), axis=0) for a_, b_ in zip(glove_2, glove_tags_2)]\n",
    "#     print(tags_1, glove_tags_1)\n",
    "    ret_1 = np.concatenate((glove_1, glove_tags_1), axis=0)\n",
    "    ret_2 = np.concatenate((glove_2, glove_tags_2), axis=0)\n",
    "    return (ret_1, ret_2)\n",
    "    \n",
    "def _get_best_tags(srl):\n",
    "    if len(srl['verbs']) == 0:\n",
    "        print(f\"Sentence without tags: {' '.join(srl['words'])}\")\n",
    "        ret = [\"O\" for w in srl['words']]\n",
    "        return ret\n",
    "    elif len(srl['verbs']) == 1:\n",
    "        return srl['verbs'][0]['tags']\n",
    "    max_tags = 0\n",
    "    final_tags = None\n",
    "    for v in srl['verbs']:\n",
    "        tag_count = 0\n",
    "        for t in v['tags']:\n",
    "            if t != 'O':\n",
    "                tag_count += 1\n",
    "        if tag_count > max_tags:\n",
    "            final_tags = v['tags']\n",
    "            max_tags = tag_count\n",
    "    if final_tags is None:\n",
    "        final_tags = srl['verbs'][0]['tags']\n",
    "    \n",
    "    return final_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_encoding_vocab(X, n_words=None):\n",
    "    wc = Counter([w[0] for pair in X for ex in pair for w in ex])\n",
    "    wc = wc.most_common(n_words) if n_words else wc.items()\n",
    "    vocab = {w for w, c in wc}\n",
    "    vocab.add(\"$UNK\")\n",
    "#     vocab.add(glove_vec(\"$UNK\", GLOVE, is_srl=True))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_srl_tag_encoding_vocab():\n",
    "    tag_file = \"/Users/bszalapski/Documents/StanfordCourses/CS224U/cs224u_project/cs224uSNLI/BiLSTM/tags.txt\"\n",
    "    tags = []\n",
    "    with open(tag_file, \"r\") as tp:\n",
    "        for l in tp.readlines():\n",
    "            tags.append(l.strip())\n",
    "    return set(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sentence_encoding_rnn(X, y):\n",
    "#     print(X)\n",
    "    vocab = get_sentence_encoding_vocab(X, n_words=10000)\n",
    "    tag_vocab = get_srl_tag_encoding_vocab()\n",
    "    mod = GloveSRLModel(\n",
    "        vocab, hidden_dim=50, max_iter=50, srl_vocab=tag_vocab, lookup=glove_lookup)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_sentence_encoding_bilstm(X, y):   \n",
    "#     vocab = get_sentence_encoding_vocab(X, n_words=10000)\n",
    "#     mod = SNLIBiLSTMAttentionClassifier(vocab)\n",
    "#     mod.fit(X, y)\n",
    "#     return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7c86e1cb1272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m'raw_examples'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mraw_assess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     }\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_sentence_encoding_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_assess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_assess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-85ca4f4498a4>\u001b[0m in \u001b[0;36mfit_sentence_encoding_rnn\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtag_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_srl_tag_encoding_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     mod = GloveSRLModel(\n\u001b[0;32m----> 6\u001b[0;31m         vocab, hidden_dim=50, max_iter=50, srl_vocab=tag_vocab, lookup=glove_lookup)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_iter'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Standard SNLI trainer, using SNLI for train and validation sets\n",
    "train_reader = nli.NLIReader(os.path.join(DATA_HOME, \"snli_1.0/preprocessed_snli_1.0_train.jsonl\"))\n",
    "assess_reader = None\n",
    "# _ = nli.experiment(\n",
    "#     train_reader=preprocessed_reader, \n",
    "#     phi=sentence_encoding_rnn_phi,\n",
    "#     train_func=fit_sentence_encoding_rnn,\n",
    "#     assess_reader=None,\n",
    "#     random_state=42,\n",
    "#     vectorize=False)\n",
    "feats = []\n",
    "labels = []\n",
    "raw_examples = []\n",
    "for ex in train_reader.read():\n",
    "    feats.append(((ex.sentence1, ex.tags1), (ex.sentence2, ex.tags2)))\n",
    "    labels.append(ex.gold_label)\n",
    "    raw_examples.append((ex.sentence1, ex.sentence2))\n",
    "train = {\n",
    "    'X': feats,\n",
    "    'y': labels,\n",
    "    'raw_examples': raw_examples\n",
    "}\n",
    "X_train = train['X']\n",
    "y_train = train['y']\n",
    "if assess_reader == None:\n",
    "    X_train, X_assess, y_train, y_assess, raw_train, raw_assess = train_test_split(\n",
    "        X_train, y_train, raw_examples, train_size=0.7, test_size=None, random_state=42)\n",
    "    assess = {\n",
    "        'X': X_assess,\n",
    "        'y': y_assess,\n",
    "        'raw_examples': raw_assess\n",
    "    }\n",
    "mod = fit_sentence_encoding_rnn(X_train, y_train)\n",
    "preds = mod.predict(X_assess)\n",
    "print(classification_report(y_assess, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addamod_train_reader = nli.AddamodTrainReader(DATA_HOME)\n",
    "addamod_dev_reader = nli.AddamodDevReader(DATA_HOME)\n",
    "subobj_train_reader = nli.SubObjTrainReader(DATA_HOME)\n",
    "subobj_dev_reader = nli.SubObjDevReader(DATA_HOME)\n",
    "breaking_reader = nli.BreakingSNLIReader(DATA_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for training on SNLI, using an adversarial reader as the validation set.\n",
    "\n",
    "_ = nli.experiment(\n",
    "    train_reader=nli.SNLITrainReader(SNLI_HOME, samp_percentage=0.10), \n",
    "    phi=sentence_encoding_rnn_phi,\n",
    "    train_func=fit_sentence_encoding_rnn,\n",
    "#     assess_reader=addamod_dev_reader,\n",
    "    assess_reader=subobj_dev_reader,\n",
    "    random_state=42,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")\n",
    "predictor.predict(\n",
    "  sentence=\"Did Uriah honestly think he could beat the game in under three hours?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
